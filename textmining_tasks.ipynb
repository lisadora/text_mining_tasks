{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textmining Tasks\n",
    "\n",
    "The notebook contains the following text-mining functions:\n",
    "1. Spelling Recommender\n",
    "2. Document Similarity Detecter\n",
    "3. Topic Detecter\n",
    "4. Spam Detecter\n",
    "5. Date-Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spelling Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Spelling recommender takes a misspelled word and recommends a correctly spelled word. It finds the word in correct_spellings that has the shortest jaccard distance, and starts with the same letter as the misspelled word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_right(misspelled_word):\n",
    "    import nltk\n",
    "    from nltk.corpus import words\n",
    "    correct_spellings = words.words()\n",
    "    first_only = [i for i in correct_spellings if i[0]== misspelled_word[0]]\n",
    "    dist = [(nltk.jaccard_distance(set(nltk.ngrams(misspelled_word, n=3)),set(nltk.ngrams(a, n=3))),a) for a in  first_only]\n",
    "    return sorted(dist)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banana'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 1\n",
    "spell_right(\"bnana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coconut'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 2\n",
    "spell_right(\"cocnut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Document Similarity Detecter\n",
    "\n",
    "computes the symmetrical path similarity between two documents by \n",
    "- finding the synsets in each document with converted tags to be read in the wordnet-format\n",
    "- computing similarities using path_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_path_similarity(doc1, doc2):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    def doc_to_synsets(string):\n",
    "        tokens= nltk.word_tokenize(string) \n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        wordnet_tags = []\n",
    "        def convert_tag(tag): \n",
    "            tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "            try:\n",
    "                return tag_dict[tag[0]]\n",
    "            except KeyError:\n",
    "                return None\n",
    "        for tag in tags:\n",
    "            try:\n",
    "                wordnet_tags.append([tag[0] ,convert_tag(tag[1])])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        s1 = []\n",
    "        for tag in wordnet_tags:\n",
    "            try:\n",
    "                s1.append(wn.synsets(tag[0], pos=tag[1])[0]) \n",
    "            except Exception as err:\n",
    "                pass\n",
    "        return s1\n",
    "    synsets1 = doc_to_synsets(doc1)\n",
    "    synsets2 = doc_to_synsets(doc2)\n",
    "    def similarity_score(s1, s2):\n",
    "        max_sims=[]\n",
    "        for i in s1:\n",
    "            maxis = []\n",
    "            for y in s2:\n",
    "                sim = i.path_similarity(y)\n",
    "                maxis.append(sim)\n",
    "            maxi_i = [n for n in maxis if n is not None]\n",
    "            maxi_i2= [x for x in maxi_i if x]\n",
    "            try:\n",
    "                maxi_i3 = max(maxi_i2)\n",
    "                max_sims.append(maxi_i3)\n",
    "            except:\n",
    "                pass\n",
    "            summe=sum(max_sims)\n",
    "            maxmax = len(max_sims)\n",
    "            try:\n",
    "                index=summe/maxmax\n",
    "            except:\n",
    "                pass\n",
    "        return index\n",
    "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5485141093474428"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "doc1 = 'This is a function and we want to know if it correctly finds out the path similarity.'\n",
    "doc2 = 'I will use this function to check if the code is correct and detects the path similarity'\n",
    "document_path_similarity(doc1, doc2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Detecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data-file from newsgroups using pickle\n",
    "with open('newsgroups', 'rb') as f: \n",
    "    newsgroup_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepocessing the data using the Tfidf-Vectorizer\n",
    "#finding tokens that occur maximal in 20% of the docs and in minumum 20 of the docs, ignoring stopwords\n",
    "#only taking into account tokens with minimum 3 alphanumerical characters\n",
    "vect = TfidfVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w\\\\w+\\\\b') \n",
    "\n",
    "X = vect.fit_transform(newsgroup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buliding the lda model\n",
    "\n",
    "#creating the document term matrix\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "#creating the dictionary with the counts as keys and tikens as values\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "#fitting the model looking for 10 topics\n",
    "ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word =id_map, passes=50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.046*\"thanks\" + 0.027*\"advance\" + 0.020*\"group\" + 0.019*\"heard\" + 0.017*\"types\" + 0.014*\"answer\" + 0.014*\"question\" + 0.013*\"looking\" + 0.013*\"copy\" + 0.011*\"considering\" + 0.010*\"address\" + 0.010*\"interested\" + 0.009*\"does\" + 0.009*\"opinions\" + 0.009*\"card\" + 0.008*\"mail\" + 0.008*\"know\" + 0.008*\"theory\" + 0.008*\"wondering\" + 0.008*\"email\" + 0.007*\"faster\" + 0.007*\"controller\" + 0.007*\"program\" + 0.007*\"just\" + 0.007*\"body\" + 0.007*\"work\" + 0.007*\"says\" + 0.007*\"wouldn\" + 0.007*\"buying\" + 0.007*\"record\"'),\n",
       " (1,\n",
       "  '0.033*\"game\" + 0.026*\"team\" + 0.024*\"year\" + 0.018*\"games\" + 0.017*\"season\" + 0.017*\"players\" + 0.017*\"play\" + 0.015*\"hockey\" + 0.014*\"league\" + 0.011*\"rangers\" + 0.011*\"baseball\" + 0.011*\"teams\" + 0.010*\"toronto\" + 0.010*\"think\" + 0.010*\"division\" + 0.009*\"leafs\" + 0.009*\"fans\" + 0.009*\"played\" + 0.009*\"smith\" + 0.009*\"player\" + 0.008*\"detroit\" + 0.008*\"great\" + 0.008*\"chicago\" + 0.008*\"playoffs\" + 0.008*\"runs\" + 0.008*\"boston\" + 0.007*\"goal\" + 0.007*\"home\" + 0.007*\"pittsburgh\" + 0.007*\"series\"'),\n",
       " (2,\n",
       "  '0.047*\"simms\" + 0.021*\"deleted\" + 0.015*\"picture\" + 0.015*\"david\" + 0.014*\"ground\" + 0.013*\"stuff\" + 0.011*\"make\" + 0.011*\"memory\" + 0.011*\"talking\" + 0.010*\"reference\" + 0.010*\"players\" + 0.010*\"worth\" + 0.010*\"written\" + 0.009*\"want\" + 0.009*\"book\" + 0.008*\"contact\" + 0.008*\"just\" + 0.008*\"need\" + 0.008*\"compare\" + 0.008*\"drivers\" + 0.007*\"european\" + 0.007*\"completely\" + 0.007*\"thanks\" + 0.007*\"board\" + 0.007*\"gives\" + 0.007*\"requires\" + 0.007*\"count\" + 0.007*\"later\" + 0.007*\"changes\" + 0.006*\"current\"'),\n",
       " (3,\n",
       "  '0.029*\"april\" + 0.021*\"player\" + 0.017*\"team\" + 0.017*\"university\" + 0.016*\"philadelphia\" + 0.016*\"points\" + 0.014*\"information\" + 0.013*\"brown\" + 0.013*\"organization\" + 0.013*\"wings\" + 0.013*\"1993\" + 0.011*\"canada\" + 0.010*\"california\" + 0.009*\"list\" + 0.009*\"status\" + 0.009*\"read\" + 0.009*\"project\" + 0.008*\"reports\" + 0.008*\"chris\" + 0.008*\"internet\" + 0.008*\"good\" + 0.008*\"giving\" + 0.008*\"contact\" + 0.008*\"luck\" + 0.008*\"rate\" + 0.008*\"apparently\" + 0.008*\"addition\" + 0.007*\"ready\" + 0.007*\"center\" + 0.007*\"steve\"'),\n",
       " (4,\n",
       "  '0.025*\"thought\" + 0.021*\"finding\" + 0.020*\"posting\" + 0.017*\"replace\" + 0.017*\"today\" + 0.014*\"exist\" + 0.014*\"words\" + 0.013*\"space\" + 0.012*\"things\" + 0.012*\"ball\" + 0.011*\"know\" + 0.009*\"thing\" + 0.009*\"earlier\" + 0.008*\"leave\" + 0.008*\"signal\" + 0.008*\"come\" + 0.008*\"great\" + 0.008*\"option\" + 0.008*\"easy\" + 0.007*\"rules\" + 0.007*\"remember\" + 0.007*\"yeah\" + 0.007*\"display\" + 0.007*\"terms\" + 0.006*\"talk\" + 0.006*\"couldn\" + 0.006*\"devices\" + 0.006*\"1990\" + 0.006*\"wish\" + 0.006*\"times\"'),\n",
       " (5,\n",
       "  '0.021*\"drive\" + 0.014*\"apple\" + 0.014*\"card\" + 0.012*\"thanks\" + 0.012*\"does\" + 0.011*\"scsi\" + 0.011*\"know\" + 0.011*\"disk\" + 0.011*\"monitor\" + 0.010*\"problem\" + 0.009*\"chip\" + 0.009*\"controller\" + 0.009*\"cable\" + 0.009*\"board\" + 0.009*\"software\" + 0.008*\"work\" + 0.008*\"port\" + 0.008*\"video\" + 0.008*\"drives\" + 0.007*\"available\" + 0.007*\"computer\" + 0.007*\"help\" + 0.007*\"light\" + 0.007*\"serial\" + 0.007*\"modem\" + 0.007*\"motherboard\" + 0.007*\"floppy\" + 0.007*\"using\" + 0.007*\"need\" + 0.006*\"mail\"'),\n",
       " (6,\n",
       "  '0.026*\"orbit\" + 0.022*\"shuttle\" + 0.018*\"cheers\" + 0.018*\"nasa\" + 0.016*\"launch\" + 0.015*\"space\" + 0.015*\"sure\" + 0.013*\"hello\" + 0.012*\"anybody\" + 0.012*\"robert\" + 0.012*\"thanks\" + 0.012*\"true\" + 0.011*\"course\" + 0.011*\"moon\" + 0.010*\"radio\" + 0.010*\"available\" + 0.010*\"just\" + 0.010*\"subject\" + 0.010*\"older\" + 0.009*\"cost\" + 0.009*\"life\" + 0.009*\"real\" + 0.009*\"public\" + 0.008*\"data\" + 0.008*\"remember\" + 0.008*\"info\" + 0.008*\"post\" + 0.008*\"michael\" + 0.008*\"kind\" + 0.007*\"general\"'),\n",
       " (7,\n",
       "  '0.009*\"just\" + 0.008*\"think\" + 0.008*\"time\" + 0.008*\"good\" + 0.008*\"people\" + 0.007*\"know\" + 0.006*\"does\" + 0.005*\"want\" + 0.005*\"really\" + 0.005*\"bike\" + 0.005*\"right\" + 0.005*\"little\" + 0.005*\"make\" + 0.004*\"used\" + 0.004*\"probably\" + 0.004*\"look\" + 0.004*\"years\" + 0.004*\"things\" + 0.004*\"space\" + 0.004*\"problem\" + 0.004*\"better\" + 0.004*\"different\" + 0.004*\"long\" + 0.004*\"going\" + 0.004*\"point\" + 0.004*\"need\" + 0.004*\"didn\" + 0.004*\"thing\" + 0.004*\"actually\" + 0.004*\"doesn\"'),\n",
       " (8,\n",
       "  '0.025*\"right\" + 0.019*\"digital\" + 0.017*\"believe\" + 0.017*\"reply\" + 0.017*\"just\" + 0.015*\"claim\" + 0.015*\"thing\" + 0.014*\"auto\" + 0.013*\"clock\" + 0.013*\"output\" + 0.012*\"thought\" + 0.012*\"wrong\" + 0.012*\"limit\" + 0.012*\"hope\" + 0.011*\"does\" + 0.010*\"recall\" + 0.009*\"message\" + 0.009*\"going\" + 0.009*\"folks\" + 0.009*\"current\" + 0.009*\"claims\" + 0.008*\"sound\" + 0.008*\"post\" + 0.008*\"correct\" + 0.008*\"training\" + 0.008*\"definitely\" + 0.008*\"know\" + 0.008*\"power\" + 0.008*\"memory\" + 0.007*\"second\"'),\n",
       " (9,\n",
       "  '0.035*\"pitt\" + 0.034*\"banks\" + 0.030*\"gordon\" + 0.027*\"surrender\" + 0.027*\"n3jxp\" + 0.027*\"chastity\" + 0.027*\"skepticism\" + 0.027*\"shameful\" + 0.027*\"cadre\" + 0.027*\"intellect\" + 0.023*\"soon\" + 0.013*\"long\" + 0.011*\"wants\" + 0.011*\"seeing\" + 0.010*\"damn\" + 0.009*\"doing\" + 0.008*\"technology\" + 0.008*\"student\" + 0.008*\"believe\" + 0.006*\"follow\" + 0.006*\"ones\" + 0.006*\"obviously\" + 0.006*\"right\" + 0.006*\"known\" + 0.006*\"agree\" + 0.005*\"guys\" + 0.005*\"probably\" + 0.005*\"case\" + 0.005*\"time\" + 0.004*\"highly\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the 10 topics with the 30 most fequent tokens\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=30)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spam Detecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset with labeled spam and nonspam data\n",
    "spam_data = pd.read_csv('spam2.csv',sep=\"\\t\", quotechar = \"|\")\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Spam 13.18858783420061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert labels to numbers 0,1\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_percent = spam_data[\"target\"].value_counts()[1]/len(spam_data[\"target\"])*100\n",
    "print(\"%Spam\", spam_percent)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9663809628462937"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using a count-vectorizer to preprocess the data\n",
    "vect = CountVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "#fitting a multinomial Naive Bayes classifier model with alpha=0.1\n",
    "MNNB = MultinomialNB(alpha = 0.1)\n",
    "MNNB.fit(X_train_vectorized, y_train)\n",
    "predictions = MNNB.predict(X_test_vectorized)\n",
    "#calculating the area under the curve (AUC) \n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking if additional parameters can add to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonspam average lenght: 71.1486151302191 spam average lenght: 139.19727891156464\n",
      "nonspam average digits: 0.32348077718065316 spam average digits: 15.851700680272108\n",
      "nonspam average non-alphanumeric characters: 29.058503401360543 spam average non-alphanumeric characters: 17.365440264572136\n"
     ]
    }
   ],
   "source": [
    "#adding additional parameters\n",
    "spam_data[\"lenght\"] = spam_data['text'].apply(lambda x: len(x))\n",
    "spam_data[\"digits\"] = spam_data['text'].str.count('\\d') \n",
    "spam_data[\"non-word\"] = spam_data['text'].str.count('\\W') \n",
    "spam = spam_data[spam_data[\"target\"] ==1]\n",
    "nonspam = spam_data[spam_data[\"target\"] !=1]\n",
    "\n",
    "#mean lenght of the data - spam versus non-spam\n",
    "spam_av = spam[\"lenght\"].mean()\n",
    "nonspam_av = nonspam[\"lenght\"].mean()\n",
    "print(\"nonspam average lenght:\", nonspam_av, \"spam average lenght:\", spam_av)\n",
    "\n",
    "#mean number of digits in text - spam versus non-spam\n",
    "spam_av_d = spam[\"digits\"].mean()\n",
    "nonspam_av_d = nonspam[\"digits\"].mean()\n",
    "print(\"nonspam average digits:\", nonspam_av_d, \"spam average digits:\", spam_av_d )\n",
    "\n",
    "#mean number of non-alphanumeric characters - spam versus nonspam\n",
    "spam_av_nonw = spam[\"non-word\"].mean()\n",
    "nonspam_av_nonw = nonspam[\"non-word\"].mean()\n",
    "print(\"nonspam average non-alphanumeric characters:\", spam_av_nonw, \"spam average non-alphanumeric characters:\", nonspam_av_nonw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for adding features in the textmatrix\n",
    "def add_feature(X, feature_to_add):\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model with the added Parameters\n",
    "X_train_n= X_train.to_frame()\n",
    "X_train_chars = X_train_n['text'].apply(lambda x: len(x))\n",
    "X_train_dig = X_train_n['text'].str.count('\\d') \n",
    "X_train_wob = X_train_n['text'].str.count('\\W') \n",
    "X_test_n =  X_test.to_frame()\n",
    "X_test_chars = X_test_n['text'].apply(lambda x: len(x))\n",
    "X_test_dig = X_test_n['text'].str.count('\\d') \n",
    "X_test_wob= X_test_n['text'].str.count('\\W') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xneu_train = add_feature(X_train_vectorized, [X_train_chars, X_train_dig, X_train_wob] )\n",
    "Xneu_test = add_feature(X_test_vectorized, [X_test_chars, X_test_dig, X_test_wob])\n",
    "Xtrain_final = Xneu_train.toarray()\n",
    "Xtest_final = Xneu_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777277185613876"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNNB = MultinomialNB(alpha = 0.1)\n",
    "MNNB.fit(Xtrain_final,y_train)\n",
    "predictions = MNNB.predict(Xtest_final)\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Date-Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Parser finds all Dates in formats like:\n",
    "\n",
    "* 04/20/2009; 04/20/09; 4/20/09; 4/3/09...\n",
    "* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009...\n",
    "* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009...\n",
    "* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009...\n",
    "* Feb 2009; Sep 2009; Oct 2010...\n",
    "* 6/2008; 12/2009...\n",
    "* 5 B.C.; 50 BC; 500 A.D.; 500 AD; A.D. 50; BC 500...\n",
    "* 2009; 1983...\n",
    "\n",
    "The function returns a dataframe including the following colums:\n",
    "* day\n",
    "* month\n",
    "* year\n",
    "* BC_AD\n",
    "* text_before_date -  gets only filled, if there is only the year provided, to avoid confusion with other numbers.\n",
    "* text_after_date - gets only filled, if there is only the year provided, to avoid confusion with other numbers\n",
    "\n",
    "\n",
    "Missing information about any of the columns content is marked as NAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parser(list_of_strings):\n",
    "    import pandas as pd\n",
    "    s = pd.Series(list_of_strings)\n",
    "    df= s.to_frame()\n",
    " \n",
    "    dfe0 = df[0].str.extract(r'(?P<month>Jan|Feb|Mar|March|Apr|April|May|Jun|June|Jul|July|Aug|Sep|Oct|Nov|Dec|December|Decemeber)\\D{,10}(?P<day>\\d?\\d)\\D\\D?\\D?\\D?(?P<year>\\d{2,4})')\n",
    "    dfe0_neu = dfe0.dropna(axis=0, how='all')\n",
    "    a = dfe0_neu.index.tolist()\n",
    "    dfa = df.drop(df.index[(a)])\n",
    "    \n",
    "    dfe1 = dfa[0].str.extract(r'(?P<day>\\d?\\d)\\D(?P<month>Jan|Feb|Mar|Apr|May|Jun|Jul|July|Aug|Sep|Oct|Nov|Dec|December|Decemeber)\\D{,10}(?P<year>\\d{2,4})')\n",
    "    dfe1_neu = dfe1.dropna(axis=0, how='all')\n",
    "    b= dfe1_neu.index.tolist()\n",
    "    dfb = dfa.drop(df.index[(b)])\n",
    "    \n",
    "    dfe2= dfb[0].str.extract(r'(?P<month>Jan|Feb|Mar|Apr|May|Jun|Jul|July|Aug|Sep|Oct|Nov|Dec)\\D{,10}(?P<day>)(?P<year>\\d{2,4})')\n",
    "    dfe2_neu = dfe2.dropna(axis=0, how='all')\n",
    "    c= dfe2_neu.index.tolist()\n",
    "    dfc = dfb.drop(df.index[(c)])\n",
    "    \n",
    "    dfe3 = dfc[0].str.extract(r'(?P<month>\\d?\\d)(/|-)(?P<day>\\d?\\d)(/|-)(?P<year>\\d{2,4})')\n",
    "    dfe3_neu = dfe3.dropna(axis=0, how='all')\n",
    "    d= dfe3_neu.index.tolist()\n",
    "    dfd = dfc.drop(df.index[(d)])\n",
    "    dfe3_neu.drop([1,3], axis=1, inplace=True)\n",
    "    \n",
    "    dfe4 = dfd[0].str.extract(r'(?P<month>\\d?\\d)(/|-)(?P<day>)(?P<year>\\d{2,4})')\n",
    "    dfe4_neu = dfe4.dropna(axis=0, how='all')\n",
    "    dfe4_neu.drop([1], axis=1, inplace=True)\n",
    "    e = dfe4_neu.index.tolist()\n",
    "    dfe = dfd.drop(df.index[(e)])\n",
    "    \n",
    "    #dates with BC AD before the date\n",
    "    dfBCAD = dfe[0].str.extract(r'(?P<month>)(?P<day>)(?P<BC_AD>[ACBD.]{2,4}) (?P<year>\\d{1,4})')\n",
    "    dfBCAD_neu = dfBCAD.dropna(axis=0, how='all')\n",
    "    f = dfBCAD_neu.index.tolist()\n",
    "    dff = dfe.drop(df.index[(f)])\n",
    "    \n",
    "    #dates with BC AD after the date\n",
    "    dfBCAD2 = dff[0].str.extract(r'(?P<month>)(?P<day>)(?P<year>\\d{1,4}) (?P<BC_AD>[ACBD.]{2,4})')\n",
    "    dfBCAD2_neu = dfBCAD2.dropna(axis=0, how='all')\n",
    "    g = dfBCAD2_neu.index.tolist()\n",
    "    dfg = dff.drop(df.index[(g)])\n",
    "    \n",
    "    #only dates provided\n",
    "    df_dates_only = dfg[0].str.extract(r'(?P<text_before_date>\\D{0,20})(?P<year>\\d{4})(?P<text_after_date>\\D{0,20})')\n",
    "    df_dates_only_neu = df_dates_only.dropna(axis=0, how='all')\n",
    "   \n",
    "    frames = [dfe0_neu, dfe1_neu, dfe2_neu, dfe3_neu, dfe4_neu, dfBCAD_neu, dfBCAD2_neu, df_dates_only_neu] #, dfBCAD2_neu\n",
    "    final = pd.concat(frames, sort=True)\n",
    "    #final['year'] = final['year'].astype('int64')\n",
    " \n",
    "    nums = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    month = ['Jan', 'Feb', 'Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "   \n",
    "    for i,y in zip(nums, month):\n",
    "        final[\"month\"].replace(y, i, inplace = True)##\n",
    "    \n",
    "    import numpy as np\n",
    "    for i in final.columns:\n",
    "        final[i].replace('', np.nan, inplace = True)\n",
    "\n",
    "    #reorder columns\n",
    "    cols = list(final)\n",
    "    for i in ['day', 'month', 'year', 'text_after_date']:\n",
    "        cols.remove(i)\n",
    "    cols.insert(0,'day')\n",
    "    cols.insert(1,'month')\n",
    "    cols.insert(2,'year')\n",
    "    cols.insert(5,'text_after_date')\n",
    "    final= final.loc[:, cols] \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>BC_AD</th>\n",
       "      <th>text_before_date</th>\n",
       "      <th>text_after_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>07</td>\n",
       "      <td>1871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>B.C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>AD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it happened suddenl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day month  year BC_AD  text_before_date       text_after_date\n",
       "4   20     3  2010   NaN               NaN                   NaN\n",
       "0   18     6    85   NaN               NaN                   NaN\n",
       "1    8    07  1871   NaN               NaN                   NaN\n",
       "2  NaN   NaN   215  B.C.               NaN                   NaN\n",
       "5  NaN   NaN    23    AD               NaN                   NaN\n",
       "3  NaN   NaN  1985   NaN               NaN   it happened suddenl"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the parser\n",
    "test_list = ['6/18/85 the primary Care Doctor found....','she plans to move in 07/8/1871 to In-Home Services', 'B.C. 215 he wondered if....', '1985 it happened suddenly....', 'In Mar 20th 2010 we met in a cafeteria', '23 AD they met in a village']\n",
    "date_parser(test_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
